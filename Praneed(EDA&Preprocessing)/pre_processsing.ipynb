{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import signal\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "class MovieDataEnricher:\n",
    "    \"\"\"\n",
    "    Enriches a movie dataset with detailed information from online sources.\n",
    "    Supports pausing and resuming the enrichment process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the enricher with an OMDb API key.\n",
    "        If no API key is provided, will use only web scraping.\n",
    "        \n",
    "        Get a free API key at: http://www.omdbapi.com/apikey.aspx\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Set a user agent to avoid being blocked\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "        }\n",
    "        self.session.headers.update(self.headers)\n",
    "        \n",
    "        # Track statistics\n",
    "        self.stats = {\n",
    "            'total_movies': 0,\n",
    "            'api_success': 0,\n",
    "            'web_success': 0,\n",
    "            'failed': 0,\n",
    "            'processed': 0\n",
    "        }\n",
    "        \n",
    "        # For pause/resume functionality\n",
    "        self.checkpoint_file = None\n",
    "        self.input_path = None\n",
    "        self.output_path = None\n",
    "        self.is_paused = False\n",
    "        self.processed_indices = set()\n",
    "    \n",
    "    def _extract_year(self, date_value):\n",
    "        \"\"\"\n",
    "        Extract year from various date formats.\n",
    "        \n",
    "        Args:\n",
    "            date_value: Date in various formats (string, datetime, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Integer year or None if extraction fails\n",
    "        \"\"\"\n",
    "        if pd.isna(date_value):\n",
    "            return None\n",
    "            \n",
    "        # If already an integer\n",
    "        if isinstance(date_value, int) and 1900 <= date_value <= 2030:\n",
    "            return date_value\n",
    "            \n",
    "        # If a string, try various formats\n",
    "        if isinstance(date_value, str):\n",
    "            # Try direct year extraction (e.g., \"2015\")\n",
    "            if re.match(r'^(19|20)\\d{2}$', date_value):\n",
    "                return int(date_value)\n",
    "                \n",
    "            # Try format like \"16-Dec-15\"\n",
    "            try:\n",
    "                dt = datetime.strptime(date_value, '%d-%b-%y')\n",
    "                return dt.year\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "            # Try format like \"2015-12-16\"\n",
    "            try:\n",
    "                dt = datetime.strptime(date_value, '%Y-%m-%d')\n",
    "                return dt.year\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "            # Try format like \"12/16/2015\"\n",
    "            try:\n",
    "                dt = datetime.strptime(date_value, '%m/%d/%Y')\n",
    "                return dt.year\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "            # Extract 4-digit number from string\n",
    "            year_match = re.search(r'(19|20)\\d{2}', date_value)\n",
    "            if year_match:\n",
    "                return int(year_match.group(0))\n",
    "        \n",
    "        # If datetime object\n",
    "        if isinstance(date_value, (datetime, pd.Timestamp)):\n",
    "            return date_value.year\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _save_checkpoint(self, movies_df):\n",
    "        \"\"\"Save the current state to allow resuming later\"\"\"\n",
    "        if self.checkpoint_file:\n",
    "            checkpoint_data = {\n",
    "                'processed_indices': self.processed_indices,\n",
    "                'stats': self.stats,\n",
    "                'last_processed_index': max(self.processed_indices) if self.processed_indices else -1\n",
    "            }\n",
    "            \n",
    "            # Save the current DataFrame\n",
    "            movies_df.to_csv(self.output_path + '.partial', index=False)\n",
    "            \n",
    "            # Save the checkpoint data\n",
    "            with open(self.checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint_data, f)\n",
    "                \n",
    "            print(f\"\\nCheckpoint saved: Processed {len(self.processed_indices)} of {self.stats['total_movies']} movies.\")\n",
    "    \n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\"Load the checkpoint data to resume processing\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                \n",
    "                self.processed_indices = checkpoint_data['processed_indices']\n",
    "                self.stats = checkpoint_data['stats']\n",
    "                \n",
    "                print(f\"Loaded checkpoint: Previously processed {len(self.processed_indices)} movies.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def _handle_interrupt(self, signal, frame):\n",
    "        \"\"\"Handle keyboard interrupt (Ctrl+C)\"\"\"\n",
    "        print(\"\\n\\nPausing the enrichment process...\")\n",
    "        self.is_paused = True\n",
    "        \n",
    "        # Let the main loop handle the actual saving\n",
    "    \n",
    "    def enrich_dataset(self, input_path, output_path, title_column='movie_title', year_column=None, limit=None, resume=True, checkpoint_interval=10):\n",
    "        \"\"\"\n",
    "        Enriches movies dataset with detailed information.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to the input CSV file\n",
    "            output_path: Path to save the enriched CSV file\n",
    "            title_column: Column name containing movie titles\n",
    "            year_column: Optional column name containing release years\n",
    "            limit: Optional limit on number of movies to process (for testing)\n",
    "            resume: Whether to try to resume from a previous run\n",
    "            checkpoint_interval: How often to save checkpoints (number of movies)\n",
    "        \n",
    "        Returns:\n",
    "            Pandas DataFrame with enriched movie data\n",
    "        \"\"\"\n",
    "        # Set up checkpoint file\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.checkpoint_file = output_path + '.checkpoint'\n",
    "        \n",
    "        # Set up signal handler\n",
    "        signal.signal(signal.SIGINT, self._handle_interrupt)\n",
    "        \n",
    "        # Check for resumable session\n",
    "        if resume and os.path.exists(self.output_path + '.partial') and self._load_checkpoint():\n",
    "            print(\"Resuming from previous session...\")\n",
    "            movies_df = pd.read_csv(self.output_path + '.partial')\n",
    "        else:\n",
    "            print(\"Starting new enrichment process...\")\n",
    "            # Reset state\n",
    "            self.processed_indices = set()\n",
    "            self.stats = {\n",
    "                'total_movies': 0,\n",
    "                'api_success': 0,\n",
    "                'web_success': 0,\n",
    "                'failed': 0,\n",
    "                'processed': 0\n",
    "            }\n",
    "            \n",
    "            print(\"Loading dataset...\")\n",
    "            movies_df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Ensure required columns exist\n",
    "            if title_column not in movies_df.columns:\n",
    "                title_options = [col for col in movies_df.columns if 'title' in col.lower() or 'movie' in col.lower()]\n",
    "                if title_options:\n",
    "                    title_column = title_options[0]\n",
    "                    print(f\"Using '{title_column}' as the title column\")\n",
    "                else:\n",
    "                    raise ValueError(f\"No title column found in dataset. Please specify the correct column name.\")\n",
    "            \n",
    "            # Limit dataset size if specified (for testing)\n",
    "            if limit and limit < len(movies_df):\n",
    "                movies_df = movies_df.head(limit)\n",
    "            \n",
    "            # Prepare new columns\n",
    "            new_columns = [\n",
    "                'imdb_id', 'certificate', 'director', 'imdb_rating', \n",
    "                'runtime_minutes', 'main_stars', 'genres', 'country',\n",
    "                'language', 'production_company', 'extracted_year'\n",
    "            ]\n",
    "            \n",
    "            for col in new_columns:\n",
    "                if col not in movies_df.columns:\n",
    "                    movies_df[col] = np.nan\n",
    "                    \n",
    "            # Pre-process year data\n",
    "            if year_column and year_column in movies_df.columns:\n",
    "                movies_df['extracted_year'] = movies_df[year_column].apply(self._extract_year)\n",
    "                print(f\"Extracted years from {year_column} column\")\n",
    "        \n",
    "        self.stats['total_movies'] = len(movies_df)\n",
    "        \n",
    "        # Process each movie\n",
    "        print(f\"Enriching data for {len(movies_df)} movies...\")\n",
    "        \n",
    "        # Create a list of indices to process\n",
    "        all_indices = set(movies_df.index.tolist())\n",
    "        remaining_indices = all_indices - self.processed_indices\n",
    "        \n",
    "        # Convert to list and sort for consistent processing order\n",
    "        remaining_indices = sorted(list(remaining_indices))\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=len(all_indices)) as progress_bar:\n",
    "            # Update progress bar to show already processed items\n",
    "            progress_bar.update(len(self.processed_indices))\n",
    "            \n",
    "            for index in remaining_indices:\n",
    "                # Check if we should pause\n",
    "                if self.is_paused:\n",
    "                    break\n",
    "                \n",
    "                row = movies_df.loc[index]\n",
    "                title = row[title_column]\n",
    "                \n",
    "                # Get year from extracted_year column\n",
    "                year = row['extracted_year'] if pd.notna(row['extracted_year']) else None\n",
    "                \n",
    "                # Skip if no title\n",
    "                if pd.isna(title) or not title:\n",
    "                    self.processed_indices.add(index)\n",
    "                    continue\n",
    "                    \n",
    "                # Try API first (if key is available)\n",
    "                if self.api_key:\n",
    "                    success = self._enrich_with_api(movies_df, index, title, year)\n",
    "                    if success:\n",
    "                        self.stats['api_success'] += 1\n",
    "                        # Wait to respect API rate limits\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        # Fall back to web scraping\n",
    "                        success = self._enrich_with_web_scraping(movies_df, index, title, year)\n",
    "                        if success:\n",
    "                            self.stats['web_success'] += 1\n",
    "                        else:\n",
    "                            self.stats['failed'] += 1\n",
    "                        \n",
    "                        # Wait between requests to avoid being blocked\n",
    "                        time.sleep(2)\n",
    "                else:\n",
    "                    # Just use web scraping\n",
    "                    success = self._enrich_with_web_scraping(movies_df, index, title, year)\n",
    "                    if success:\n",
    "                        self.stats['web_success'] += 1\n",
    "                    else:\n",
    "                        self.stats['failed'] += 1\n",
    "                    \n",
    "                    # Wait between requests to avoid being blocked\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                # Mark as processed\n",
    "                self.processed_indices.add(index)\n",
    "                self.stats['processed'] += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Save checkpoint occasionally\n",
    "                if self.stats['processed'] % checkpoint_interval == 0:\n",
    "                    self._save_checkpoint(movies_df)\n",
    "        \n",
    "        # Save final results\n",
    "        print(\"Saving enriched dataset...\")\n",
    "        \n",
    "        # Optionally drop the temporary extracted_year column\n",
    "        if 'extracted_year' in movies_df.columns:\n",
    "            movies_df = movies_df.drop(columns=['extracted_year'])\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Save final dataset\n",
    "        movies_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        # If completed successfully, remove checkpoint files\n",
    "        if len(self.processed_indices) == self.stats['total_movies']:\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "            if os.path.exists(self.output_path + '.partial'):\n",
    "                os.remove(self.output_path + '.partial')\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nEnrichment Statistics:\")\n",
    "        print(f\"Total movies processed: {self.stats['processed']} of {self.stats['total_movies']}\")\n",
    "        if self.api_key:\n",
    "            print(f\"Successfully enriched via API: {self.stats['api_success']} ({self.stats['api_success']/self.stats['processed']:.1%})\")\n",
    "        print(f\"Successfully enriched via web scraping: {self.stats['web_success']} ({self.stats['web_success']/self.stats['processed']:.1%})\")\n",
    "        print(f\"Failed to enrich: {self.stats['failed']} ({self.stats['failed']/self.stats['processed']:.1%})\")\n",
    "        \n",
    "        if self.is_paused:\n",
    "            print(\"\\nProcess was paused. Run the script again with resume=True to continue.\")\n",
    "        \n",
    "        return movies_df\n",
    "    \n",
    "    def _enrich_with_api(self, df, index, title, year=None):\n",
    "        \"\"\"\n",
    "        Enriches a movie using the OMDb API.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the movies\n",
    "            index: Row index to update\n",
    "            title: Movie title to search for\n",
    "            year: Optional release year\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        # Build search URL\n",
    "        params = {\n",
    "            'apikey': self.api_key,\n",
    "            't': title,\n",
    "            'plot': 'short',\n",
    "            'r': 'json'\n",
    "        }\n",
    "        \n",
    "        # Only add year if it's a valid integer\n",
    "        if year is not None and isinstance(year, (int, float)) and not np.isnan(year):\n",
    "            params['y'] = int(year)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get('http://www.omdbapi.com/', params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Check if we got a valid response\n",
    "            if data.get('Response') == 'True':\n",
    "                # Update DataFrame with API data\n",
    "                df.at[index, 'imdb_id'] = data.get('imdbID', np.nan)\n",
    "                df.at[index, 'certificate'] = data.get('Rated', np.nan)\n",
    "                df.at[index, 'director'] = data.get('Director', np.nan)\n",
    "                df.at[index, 'imdb_rating'] = data.get('imdbRating', np.nan)\n",
    "                \n",
    "                # Convert runtime to minutes\n",
    "                if 'Runtime' in data and data['Runtime'] != 'N/A':\n",
    "                    runtime_str = data['Runtime']\n",
    "                    minutes = re.search(r'(\\d+)', runtime_str)\n",
    "                    if minutes:\n",
    "                        df.at[index, 'runtime_minutes'] = int(minutes.group(1))\n",
    "                \n",
    "                df.at[index, 'main_stars'] = data.get('Actors', np.nan)\n",
    "                df.at[index, 'genres'] = data.get('Genre', np.nan)\n",
    "                df.at[index, 'country'] = data.get('Country', np.nan)\n",
    "                df.at[index, 'language'] = data.get('Language', np.nan)\n",
    "                df.at[index, 'production_company'] = data.get('Production', np.nan)\n",
    "                \n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"API error for '{title}': {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _find_imdb_id(self, title, year=None):\n",
    "        \"\"\"\n",
    "        Finds an IMDb ID by searching for a movie title.\n",
    "        \n",
    "        Args:\n",
    "            title: Movie title to search for\n",
    "            year: Optional release year\n",
    "            \n",
    "        Returns:\n",
    "            IMDb ID or None if not found\n",
    "        \"\"\"\n",
    "        search_query = title\n",
    "        if year:\n",
    "            search_query += f\" {year}\"\n",
    "            \n",
    "        search_url = f\"https://www.imdb.com/find/?q={search_query}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(search_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for search results\n",
    "            search_results = soup.select('li.ipc-metadata-list-summary-item')\n",
    "            \n",
    "            for result in search_results:\n",
    "                # Check if it's a movie/TV show result\n",
    "                if result.select_one('.ipc-metadata-list-summary-item__tl'):\n",
    "                    link = result.select_one('a')\n",
    "                    if link and 'href' in link.attrs:\n",
    "                        href = link['href']\n",
    "                        imdb_id_match = re.search(r'/title/(tt\\d+)/', href)\n",
    "                        if imdb_id_match:\n",
    "                            return imdb_id_match.group(1)\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding IMDb ID for '{title}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _enrich_with_web_scraping(self, df, index, title, year=None):\n",
    "        \"\"\"\n",
    "        Enriches a movie using web scraping from IMDb.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the movies\n",
    "            index: Row index to update\n",
    "            title: Movie title to search for\n",
    "            year: Optional release year\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        # First, try to find the IMDb ID\n",
    "        imdb_id = self._find_imdb_id(title, year)\n",
    "        \n",
    "        if not imdb_id:\n",
    "            return False\n",
    "            \n",
    "        # Store the IMDb ID\n",
    "        df.at[index, 'imdb_id'] = imdb_id\n",
    "        \n",
    "        # Get the movie page\n",
    "        movie_url = f\"https://www.imdb.com/title/{imdb_id}/\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(movie_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract data using various selectors\n",
    "            \n",
    "            # Certificate\n",
    "            certificate_elem = soup.select_one('a[href*=\"certificates\"]')\n",
    "            if certificate_elem:\n",
    "                df.at[index, 'certificate'] = certificate_elem.text.strip()\n",
    "            \n",
    "            # Director\n",
    "            director_elem = soup.select_one('a[href*=\"tt_ov_dr\"]')\n",
    "            if director_elem:\n",
    "                df.at[index, 'director'] = director_elem.text.strip()\n",
    "            else:\n",
    "                # Try alternative selector\n",
    "                directors = soup.select('.ipc-metadata-list__item:contains(\"Director\") a')\n",
    "                if directors:\n",
    "                    df.at[index, 'director'] = ', '.join([d.text.strip() for d in directors[:2]])\n",
    "            \n",
    "            # IMDb Rating\n",
    "            rating_elem = soup.select_one('.ipc-button__text .sc-bde20123-1')\n",
    "            if rating_elem:\n",
    "                df.at[index, 'imdb_rating'] = rating_elem.text.strip()\n",
    "            \n",
    "            # Runtime\n",
    "            runtime_elem = soup.select_one('span[class*=\"sc-afe43def-4\"]')\n",
    "            if runtime_elem:\n",
    "                runtime_text = runtime_elem.text.strip()\n",
    "                hours_match = re.search(r'(\\d+)h', runtime_text)\n",
    "                minutes_match = re.search(r'(\\d+)m', runtime_text)\n",
    "                \n",
    "                total_minutes = 0\n",
    "                if hours_match:\n",
    "                    total_minutes += int(hours_match.group(1)) * 60\n",
    "                if minutes_match:\n",
    "                    total_minutes += int(minutes_match.group(1))\n",
    "                \n",
    "                if total_minutes > 0:\n",
    "                    df.at[index, 'runtime_minutes'] = total_minutes\n",
    "            \n",
    "            # Stars\n",
    "            stars_elems = soup.select('.ipc-metadata-list__item:contains(\"Stars\") a.ipc-metadata-list-item__list-content-item')\n",
    "            if stars_elems:\n",
    "                stars = [s.text.strip() for s in stars_elems if not 'See full cast' in s.text]\n",
    "                df.at[index, 'main_stars'] = ', '.join(stars[:4])  # Get up to 4 stars\n",
    "            \n",
    "            # Genres\n",
    "            genre_elems = soup.select('a[href*=\"genres\"]')\n",
    "            if genre_elems:\n",
    "                genres = [g.text.strip() for g in genre_elems]\n",
    "                df.at[index, 'genres'] = ', '.join(genres)\n",
    "            \n",
    "            # Country\n",
    "            country_elem = soup.select_one('a[href*=\"country_of_origin\"]')\n",
    "            if country_elem:\n",
    "                df.at[index, 'country'] = country_elem.text.strip()\n",
    "            \n",
    "            # Language\n",
    "            language_elem = soup.select_one('a[href*=\"primary_language\"]')\n",
    "            if language_elem:\n",
    "                df.at[index, 'language'] = language_elem.text.strip()\n",
    "            \n",
    "            # Production company (harder to reliably extract)\n",
    "            company_elems = soup.select('.ipc-metadata-list__item:contains(\"Production companies\") a')\n",
    "            if company_elems:\n",
    "                companies = [c.text.strip() for c in company_elems if not 'See more' in c.text]\n",
    "                df.at[index, 'production_company'] = ', '.join(companies)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Web scraping error for '{title}' (IMDb ID: {imdb_id}): {e}\")\n",
    "            return False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input and output file paths\n",
    "    input_path = \"./raw_data/numbers1.csv\"\n",
    "    output_path = \"./processed_data/enriched_movies.csv\"\n",
    "    \n",
    "    # Create the enricher\n",
    "    # You can get a free API key at: http://www.omdbapi.com/apikey.aspx\n",
    "    enricher = MovieDataEnricher(api_key=\"ccc9c4b6\")  # Your API key\n",
    "    \n",
    "    # Check if we should resume\n",
    "    checkpoint_file = output_path + '.checkpoint'\n",
    "    resume = False\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        resume_input = input(\"Found a previous session. Do you want to resume? (y/n): \")\n",
    "        resume = resume_input.lower() == 'y'\n",
    "    \n",
    "    # Enrich the dataset\n",
    "    enriched_df = enricher.enrich_dataset(\n",
    "        input_path=input_path,\n",
    "        output_path=output_path,\n",
    "        title_column='movie_title',  # Update this to match your column name\n",
    "        year_column='release_date',  # Update this to match your column name\n",
    "        resume=resume,\n",
    "        checkpoint_interval=5  # Save checkpoint every 5 movies\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
