{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"ttps://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import logging\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Dict, Any, Tuple, Set, List\n",
    "\n",
    "# Third-Party Library\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_pickle(r'C:\\Users\\divya\\OneDrive\\Documents\\GitHub\\spring-2025-greenlight\\processed_data\\combined_dataset.pkl')\n",
    "subset_data = data.dropna(subset=['final_worldwide_boxoffice', \n",
    "                                        'final_budget', \n",
    "                                        'final_domestic_boxoffice'])\n",
    "subset_data.to_csv(r\"C:\\Users\\divya\\OneDrive\\Documents\\GitHub\\spring-2025-greenlight\\Praneed(EDA&Preprocessing)\\data.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = subset_data.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissing values by column (%):\")\n",
    "print(missing.apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(subset_data.isna(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "plt.tight_layout()\n",
    "plt.title('Missing Value Matrix (Yellow = Missing)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OMDB_API_KEY = 'ccc9c4b6'  # OMDb API key\n",
    "TMDB_API_KEY = '464f3e996afaffb22d3a2230433aa2a4'  # TMDb API key\n",
    "\n",
    "# Logger  \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# helper function \n",
    "def parse_iso8601_duration(duration_str: Optional[str]) -> Optional[int]:\n",
    "    \"\"\"Convert ISO 8601 duration (e.g., PT2H30M) to total minutes.\"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str) or not duration_str.startswith('PT'):\n",
    "        return None\n",
    "    hours, minutes = 0, 0\n",
    "    hour_match = re.search(r'(\\d+)H', duration_str)\n",
    "    minute_match = re.search(r'(\\d+)M', duration_str)\n",
    "    try:\n",
    "        if hour_match:\n",
    "            hours = int(hour_match.group(1))\n",
    "        if minute_match:\n",
    "            minutes = int(minute_match.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "    total_minutes = (hours * 60) + minutes\n",
    "    return total_minutes if total_minutes > 0 else None\n",
    "\n",
    "# main class\n",
    "class MovieDataEnricher:\n",
    "    \"\"\"\n",
    "    Enrich movie datasets using OMDb and TMDb APIs (OMDb is prioritized).\n",
    "    Features include concurrency, rate limiting, retries, caching, pause/resume, and secure API key management.\n",
    "    \"\"\"\n",
    "    ENRICHMENT_COLUMNS = [\n",
    "        'imdb_id', 'certificate', 'director', 'rating', 'runtime', 'star',\n",
    "        'genres', 'production_countries', 'original_language', 'production_companies',\n",
    "        'enrichment_source'\n",
    "    ]\n",
    "    NUMERIC_COLUMNS = ['rating', 'runtime']\n",
    "    OMDB_URL = 'http://www.omdbapi.com/'\n",
    "    TMDB_API_BASE_URL = 'https://api.themoviedb.org/3'\n",
    "\n",
    "    def __init__(self,\n",
    "                 omdb_api_key: Optional[str] = OMDB_API_KEY,\n",
    "                 tmdb_api_key: Optional[str] = TMDB_API_KEY,\n",
    "                 max_workers: int = 10,\n",
    "                 rate_limit_per_second: float = 10.0):\n",
    "        self.omdb_api_key = omdb_api_key\n",
    "        self.tmdb_api_key = tmdb_api_key\n",
    "        self.max_workers = max(1, max_workers)\n",
    "        self.rate_limit_per_second = max(0.1, rate_limit_per_second)\n",
    "        self.min_interval_api = 1.0 / self.rate_limit_per_second\n",
    "        self.last_request_time = 0.0\n",
    "        self.rate_limit_lock = threading.Lock()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        if not self.omdb_api_key:\n",
    "            self.logger.warning(\"OMDb API key missing. OMDb lookups disabled.\")\n",
    "        if not self.tmdb_api_key:\n",
    "            self.logger.warning(\"TMDb API key missing. TMDb lookups disabled.\")\n",
    "        if not self.omdb_api_key and not self.tmdb_api_key:\n",
    "            self.logger.critical(\"No API keys provided. Cannot fetch data.\")\n",
    "        self.session = self._create_session()\n",
    "        self.stats = {\n",
    "            'total_movies': 0, 'attempted': 0, 'success_omdb_id': 0, 'success_omdb_title': 0,\n",
    "            'success_tmdb': 0, 'skipped_no_title': 0, 'failed': 0, 'processed_indices': set()\n",
    "        }\n",
    "        self.checkpoint_file_base = None\n",
    "        self.is_paused = False\n",
    "        self.progress_bar: Optional[tqdm] = None\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        headers = {\n",
    "            'User-Agent': 'MovieDataEnricherScript/1.0',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        session.headers.update(headers)\n",
    "        retry_strategy = Retry(\n",
    "            total=3, backoff_factor=0.5,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(\n",
    "            max_retries=retry_strategy,\n",
    "            pool_connections=self.max_workers,\n",
    "            pool_maxsize=self.max_workers * 2\n",
    "        )\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        self.logger.info(f\"Session created with retries and a connection pool of size {self.max_workers}.\")\n",
    "        return session\n",
    "\n",
    "    def _initialize_columns(self, movies_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ensure all enrichment columns exist with the correct data types.\"\"\"\n",
    "        for col in self.ENRICHMENT_COLUMNS:\n",
    "            if col not in movies_df.columns:\n",
    "                if col in self.NUMERIC_COLUMNS:\n",
    "                    movies_df[col] = pd.Series(dtype=float)\n",
    "                else:\n",
    "                    movies_df[col] = pd.Series(dtype=object)\n",
    "            elif col in self.NUMERIC_COLUMNS:\n",
    "                movies_df[col] = pd.to_numeric(movies_df[col], errors='coerce')\n",
    "        return movies_df\n",
    "\n",
    "    @lru_cache(maxsize=2048)\n",
    "    def _extract_year(self, date_value: Any) -> Optional[int]:\n",
    "        if pd.isna(date_value):\n",
    "            return None\n",
    "        current_full_year = datetime.now().year\n",
    "        future_year_limit = current_full_year + 5\n",
    "        try:\n",
    "            if isinstance(date_value, (int, float)) and 1880 <= int(date_value) <= future_year_limit:\n",
    "                return int(date_value)\n",
    "            if isinstance(date_value, (datetime, pd.Timestamp)):\n",
    "                if 1880 <= date_value.year <= future_year_limit:\n",
    "                    return date_value.year\n",
    "            if isinstance(date_value, str):\n",
    "                date_str = date_value.strip()\n",
    "                if not date_str:\n",
    "                    return None\n",
    "                if re.fullmatch(r'(1[89]|20)\\d{2}', date_str):\n",
    "                    return int(date_str)\n",
    "                year_match = re.search(r'\\b(1[89]|20)\\d{2}\\b', date_str)\n",
    "                if year_match:\n",
    "                    return int(year_match.group(0))\n",
    "                common_formats = ('%Y-%m-%d', '%m/%d/%Y', '%d-%b-%y', '%d-%b-%Y', '%B %d, %Y', '%Y', '%b %Y', '%b-%Y')\n",
    "                for fmt in common_formats:\n",
    "                    try:\n",
    "                        dt_str = date_str.split(' ')[0].split('-')[0] if fmt == '%b-%Y' else date_str.split(' ')[0]\n",
    "                        dt = datetime.strptime(dt_str, fmt)\n",
    "                        if 1880 <= dt.year <= future_year_limit:\n",
    "                            return dt.year\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Year extraction failed for {date_value}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        with self.rate_limit_lock:\n",
    "            current_time = time.monotonic()\n",
    "            elapsed = current_time - self.last_request_time\n",
    "            wait_time = self.min_interval_api - elapsed\n",
    "            if wait_time > 0:\n",
    "                time.sleep(wait_time)\n",
    "                self.last_request_time = time.monotonic()\n",
    "            else:\n",
    "                self.last_request_time = current_time\n",
    "\n",
    "    def _save_checkpoint(self, movies_df: pd.DataFrame):\n",
    "        if not self.checkpoint_file_base:\n",
    "            return\n",
    "        checkpoint_meta_file = self.checkpoint_file_base + '.meta'\n",
    "        checkpoint_data_file = self.checkpoint_file_base + '.csv'\n",
    "        checkpoint_data = {\n",
    "            'processed_indices': list(self.stats['processed_indices']),\n",
    "            'stats': {k: v for k, v in self.stats.items() if k != 'processed_indices'}\n",
    "        }\n",
    "        try:\n",
    "            for col in self.NUMERIC_COLUMNS:\n",
    "                if col in movies_df.columns:\n",
    "                    movies_df[col] = pd.to_numeric(movies_df[col], errors='coerce')\n",
    "            movies_df.to_csv(checkpoint_data_file, index=False)\n",
    "            with open(checkpoint_meta_file, 'wb') as f:\n",
    "                pickle.dump(checkpoint_data, f)\n",
    "            self.logger.debug(f\"Checkpoint saved with {len(self.stats['processed_indices'])} processed entries.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "    def _load_checkpoint(self) -> Tuple[Optional[pd.DataFrame], bool]:\n",
    "        if not self.checkpoint_file_base:\n",
    "            return None, False\n",
    "        checkpoint_meta_file = self.checkpoint_file_base + '.meta'\n",
    "        checkpoint_data_file = self.checkpoint_file_base + '.csv'\n",
    "        if os.path.exists(checkpoint_meta_file) and os.path.exists(checkpoint_data_file):\n",
    "            try:\n",
    "                with open(checkpoint_meta_file, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                movies_df = pd.read_csv(checkpoint_data_file, low_memory=False)\n",
    "                self.stats.update(checkpoint_data.get('stats', {}))\n",
    "                default_stats = {k: 0 for k in self.stats if k != 'processed_indices'}\n",
    "                for key in default_stats:\n",
    "                    self.stats.setdefault(key, 0)\n",
    "                self.stats['processed_indices'] = set(checkpoint_data.get('processed_indices', []))\n",
    "                movies_df = self._initialize_columns(movies_df)\n",
    "                self.logger.info(f\"Loaded checkpoint with {len(self.stats['processed_indices'])} entries.\")\n",
    "                return movies_df, True\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading checkpoint: {e}. Starting fresh.\", exc_info=True)\n",
    "                if os.path.exists(checkpoint_meta_file):\n",
    "                    os.remove(checkpoint_meta_file)\n",
    "                if os.path.exists(checkpoint_data_file):\n",
    "                    os.remove(checkpoint_data_file)\n",
    "                return None, False\n",
    "        else:\n",
    "            return None, False\n",
    "\n",
    "    def _handle_interrupt(self, signum, frame):\n",
    "        if not self.is_paused:\n",
    "            self.logger.warning(\"Pause requested. Finishing current batch and saving checkpoint...\")\n",
    "            self.is_paused = True\n",
    "            signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "            if self.progress_bar:\n",
    "                self.progress_bar.set_description(\"Pausing...\")\n",
    "\n",
    "    def _cleanup_checkpoints(self):\n",
    "        if self.checkpoint_file_base:\n",
    "            meta_file = self.checkpoint_file_base + '.meta'\n",
    "            data_file = self.checkpoint_file_base + '.csv'\n",
    "            removed_count = 0\n",
    "            try:\n",
    "                if os.path.exists(meta_file):\n",
    "                    os.remove(meta_file)\n",
    "                    removed_count += 1\n",
    "                if os.path.exists(data_file):\n",
    "                    os.remove(data_file)\n",
    "                    removed_count += 1\n",
    "                if removed_count > 0:\n",
    "                    self.logger.info(f\"Removed {removed_count} checkpoint file(s).\")\n",
    "            except OSError as e:\n",
    "                self.logger.warning(f\"Could not remove checkpoint files: {e}\")\n",
    "\n",
    "    @lru_cache(maxsize=2048)\n",
    "    def _find_tmdb_id(self, title: str, year: Optional[int] = None) -> Optional[int]:\n",
    "        \"\"\"Search TMDb API for a movie ID.\"\"\"\n",
    "        if not self.tmdb_api_key or not title:\n",
    "            return None\n",
    "        search_url = f\"{self.TMDB_API_BASE_URL}/search/movie\"\n",
    "        params = {\n",
    "            'api_key': self.tmdb_api_key,\n",
    "            'query': title,\n",
    "            'language': 'en-US',\n",
    "            'include_adult': 'false'\n",
    "        }\n",
    "        log_identifier = f\"'{title}'\"\n",
    "        if year:\n",
    "            params['primary_release_year'] = year\n",
    "            log_identifier += f\" ({year})\"\n",
    "        self._rate_limit()\n",
    "        try:\n",
    "            response = self.session.get(search_url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            results = data.get('results')\n",
    "            if results:\n",
    "                if year:\n",
    "                    for res in results:\n",
    "                        rd = res.get('release_date')\n",
    "                        if rd and str(year) in rd:\n",
    "                            return res.get('id')\n",
    "                self.logger.debug(f\"TMDb ID {results[0].get('id')} found for {log_identifier}.\")\n",
    "                return results[0].get('id')\n",
    "            else:\n",
    "                self.logger.debug(f\"TMDb search did not find {log_identifier}.\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.warning(f\"TMDb search network error for {log_identifier}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"TMDb search error for {log_identifier}: {e}\", exc_info=False)\n",
    "            return None\n",
    "\n",
    "    def _enrich_with_omdb_api(self, title: Optional[str] = None, year: Optional[int] = None, imdb_id: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Fetch movie data from OMDb API.\"\"\"\n",
    "        if not self.omdb_api_key or (not imdb_id and not title):\n",
    "            return None\n",
    "        params = {'apikey': self.omdb_api_key, 'plot': 'short', 'r': 'json'}\n",
    "        log_identifier = \"\"\n",
    "        source = 'failed'\n",
    "        if imdb_id:\n",
    "            params['i'] = imdb_id\n",
    "            log_identifier = f\"IMDb ID {imdb_id}\"\n",
    "            source = 'omdb_id'\n",
    "        elif title:\n",
    "            params['t'] = title\n",
    "            params['type'] = 'movie'\n",
    "            log_identifier = f\"'{title}'\"\n",
    "            source = 'omdb_title'\n",
    "        if title and year:\n",
    "            params['y'] = year\n",
    "            log_identifier += f\" ({year})\"\n",
    "        if 't' not in params and 'i' not in params:\n",
    "            return None\n",
    "        self._rate_limit()\n",
    "        try:\n",
    "            response = self.session.get(self.OMDB_URL, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data.get('Response') == 'True':\n",
    "                movie_data = {col: np.nan for col in self.ENRICHMENT_COLUMNS}\n",
    "                movie_data.update({\n",
    "                    'imdb_id': data.get('imdbID'),\n",
    "                    'certificate': data.get('Rated'),\n",
    "                    'director': data.get('Director'),\n",
    "                    'rating': pd.to_numeric(data.get('imdbRating'), errors='coerce'),\n",
    "                    'star': data.get('Actors'),\n",
    "                    'genres': data.get('Genre'),\n",
    "                    'production_countries': data.get('Country'),\n",
    "                    'original_language': data.get('Language'),\n",
    "                    'production_companies': data.get('Production'),\n",
    "                    'enrichment_source': source\n",
    "                })\n",
    "                runtime_str = data.get('Runtime')\n",
    "                if runtime_str and runtime_str != 'N/A':\n",
    "                    m = re.search(r'(\\d+)', runtime_str)\n",
    "                    movie_data['runtime'] = int(m.group(1)) if m else np.nan\n",
    "                for k, v in movie_data.items():\n",
    "                    if isinstance(v, str) and v.strip().upper() in ['N/A', 'NONE', '']:\n",
    "                        movie_data[k] = np.nan\n",
    "                    elif k in self.NUMERIC_COLUMNS and v == 0:\n",
    "                        movie_data[k] = np.nan\n",
    "                return movie_data\n",
    "            else:\n",
    "                error_msg = data.get('Error', 'Unknown')\n",
    "                if source == 'omdb_id' or \"not found\" not in error_msg.lower():\n",
    "                    self.logger.debug(f\"OMDb API error for {log_identifier}: {error_msg}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.warning(f\"OMDb API network error for {log_identifier}: {e}\")\n",
    "            return None\n",
    "        except ValueError as e:\n",
    "            self.logger.warning(f\"OMDb JSON decode error for {log_identifier}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"OMDb API unexpected error for {log_identifier}: {e}\", exc_info=False)\n",
    "            return None\n",
    "\n",
    "    def _enrich_with_tmdb_api(self, tmdb_id: int) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Fetch movie data from TMDb API.\"\"\"\n",
    "        if not self.tmdb_api_key or not tmdb_id:\n",
    "            return None\n",
    "        details_url = f\"{self.TMDB_API_BASE_URL}/movie/{tmdb_id}\"\n",
    "        params = {\n",
    "            'api_key': self.tmdb_api_key,\n",
    "            'language': 'en-US',\n",
    "            'append_to_response': 'credits,release_dates'\n",
    "        }\n",
    "        self._rate_limit()\n",
    "        try:\n",
    "            response = self.session.get(details_url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            movie_data = {col: np.nan for col in self.ENRICHMENT_COLUMNS}\n",
    "            movie_data['enrichment_source'] = 'tmdb'\n",
    "            movie_data['imdb_id'] = data.get('imdb_id') or np.nan\n",
    "            movie_data['runtime'] = data.get('runtime')\n",
    "            movie_data['genres'] = ', '.join([g['name'] for g in data.get('genres', [])])\n",
    "            spoken_langs = data.get('spoken_languages', [])\n",
    "            movie_data['original_language'] = spoken_langs[0]['english_name'] if spoken_langs else np.nan\n",
    "            movie_data['production_countries'] = ', '.join([c['name'] for c in data.get('production_countries', [])])\n",
    "            movie_data['production_companies'] = ', '.join([c['name'] for c in data.get('production_companies', [])])\n",
    "            movie_data['rating'] = data.get('vote_average')\n",
    "            credits = data.get('credits', {})\n",
    "            cast = credits.get('cast', [])\n",
    "            crew = credits.get('crew', [])\n",
    "            if cast:\n",
    "                movie_data['star'] = ', '.join([a['name'] for a in cast[:4]])\n",
    "            directors = [p['name'] for p in crew if p.get('job') == 'Director']\n",
    "            movie_data['director'] = ', '.join(directors) if directors else np.nan\n",
    "            release_dates = data.get('release_dates', {}).get('results', [])\n",
    "            us_cert = None\n",
    "            for rd in release_dates:\n",
    "                if rd.get('iso_3166_1') == 'US':\n",
    "                    for r in rd.get('release_dates', []):\n",
    "                        cert = r.get('certification')\n",
    "                        if cert:\n",
    "                            us_cert = cert\n",
    "                            break\n",
    "                if us_cert:\n",
    "                    break\n",
    "            movie_data['certificate'] = us_cert\n",
    "            for k, v in movie_data.items():\n",
    "                if v is None or (isinstance(v, (str, list, dict)) and not v):\n",
    "                    movie_data[k] = np.nan\n",
    "                elif k in self.NUMERIC_COLUMNS and v == 0:\n",
    "                    movie_data[k] = np.nan\n",
    "            return movie_data\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.warning(f\"TMDb details network error ID {tmdb_id}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"TMDb details error ID {tmdb_id}: {e}\", exc_info=False)\n",
    "            return None\n",
    "\n",
    "    def _process_movie(self, args: Tuple[int, pd.Series, str, Optional[str]]) -> Tuple[int, Optional[Dict[str, Any]], str]:\n",
    "        \"\"\"Process a single movie using OMDb and TMDb APIs.\"\"\"\n",
    "        index, row, title_column, _ = args\n",
    "        title = row.get(title_column)\n",
    "        if pd.isna(title) or not isinstance(title, str) or not title.strip():\n",
    "            return index, None, 'skipped_no_title'\n",
    "        title = title.strip()\n",
    "        year = row.get('extracted_year')\n",
    "        result_data: Optional[Dict[str, Any]] = None\n",
    "        source: str = 'failed'\n",
    "        existing_imdb_id = row.get('_input_imdb_id')\n",
    "        if pd.isna(existing_imdb_id) or not isinstance(existing_imdb_id, str) or not existing_imdb_id.startswith('tt'):\n",
    "            existing_imdb_id = None\n",
    "\n",
    "        # Attempt enrichment using OMDb API first\n",
    "        if self.omdb_api_key:\n",
    "            if existing_imdb_id:\n",
    "                result_data = self._enrich_with_omdb_api(imdb_id=existing_imdb_id)\n",
    "            if not result_data:\n",
    "                result_data = self._enrich_with_omdb_api(title=title, year=year)\n",
    "            if result_data:\n",
    "                source = result_data.get('enrichment_source', 'omdb_title')\n",
    "\n",
    "        # Fallback to TMDb API if OMDb fails\n",
    "        if not result_data and self.tmdb_api_key:\n",
    "            tmdb_id = self._find_tmdb_id(title, year)\n",
    "            if tmdb_id:\n",
    "                result_data = self._enrich_with_tmdb_api(tmdb_id)\n",
    "            if result_data:\n",
    "                source = result_data.get('enrichment_source', 'tmdb')\n",
    "\n",
    "        if result_data:\n",
    "            final_imdb_id = result_data.get('imdb_id')\n",
    "            if source == 'tmdb' and pd.isna(final_imdb_id) and existing_imdb_id:\n",
    "                result_data['imdb_id'] = existing_imdb_id\n",
    "            elif source != 'tmdb' and existing_imdb_id and pd.isna(final_imdb_id):\n",
    "                result_data['imdb_id'] = existing_imdb_id\n",
    "            return index, result_data, source\n",
    "        else:\n",
    "            self.logger.debug(f\"Failed to enrich '{title}' ({year}) using APIs.\")\n",
    "            return index, None, 'failed'\n",
    "\n",
    "    def enrich_dataset(self,\n",
    "                       input_path: str,\n",
    "                       output_path: str,\n",
    "                       title_column: str = 'movie_title',\n",
    "                       year_column: Optional[str] = None,\n",
    "                       imdb_id_column: Optional[str] = 'imdb_id',\n",
    "                       limit: Optional[int] = None,\n",
    "                       resume: bool = True,\n",
    "                       checkpoint_interval: int = 50,\n",
    "                       batch_size: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Load, enrich, and save the movie dataset.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.is_paused = False\n",
    "        output_dir = os.path.dirname(output_path) or '.'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.checkpoint_file_base = os.path.join(output_dir, os.path.basename(output_path) + \".checkpoint\")\n",
    "        initial_processed_count = 0\n",
    "        signal.signal(signal.SIGINT, self._handle_interrupt)\n",
    "\n",
    "        movies_df: Optional[pd.DataFrame] = None\n",
    "        loaded_from_checkpoint = False\n",
    "        if resume:\n",
    "            movies_df, loaded_from_checkpoint = self._load_checkpoint()\n",
    "        if loaded_from_checkpoint and movies_df is not None:\n",
    "            initial_processed_count = len(self.stats['processed_indices'])\n",
    "\n",
    "        if not loaded_from_checkpoint or movies_df is None:\n",
    "            self.logger.info(\"Starting a new enrichment process.\")\n",
    "            self.stats = {k: (set() if k == 'processed_indices' else 0) for k in self.stats.keys()}\n",
    "            initial_processed_count = 0\n",
    "            try:\n",
    "                self.logger.info(f\"Loading dataset from: {input_path}\")\n",
    "                movies_df = pd.read_csv(input_path, low_memory=False)\n",
    "                self.logger.info(f\"Dataset loaded with {len(movies_df)} rows.\")\n",
    "            except FileNotFoundError:\n",
    "                self.logger.critical(f\"Input file not found: {input_path}\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                self.logger.critical(f\"Error loading input CSV {input_path}: {e}\", exc_info=True)\n",
    "                raise\n",
    "\n",
    "            if title_column not in movies_df.columns:\n",
    "                possible_titles = [c for c in movies_df.columns if 'title' in c.lower() or 'movie' in c.lower() or 'name' in c.lower()]\n",
    "                if possible_titles:\n",
    "                    title_column = possible_titles[0]\n",
    "                    self.logger.info(f\"Auto-detected title column: '{title_column}'\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Title column '{title_column}' not found.\")\n",
    "            if year_column and year_column not in movies_df.columns:\n",
    "                self.logger.warning(f\"Year column '{year_column}' not found.\")\n",
    "                year_column = None\n",
    "            if imdb_id_column and imdb_id_column not in movies_df.columns:\n",
    "                self.logger.info(f\"Input IMDb ID column '{imdb_id_column}' not found.\")\n",
    "                imdb_id_column = None\n",
    "\n",
    "            if limit and limit < len(movies_df):\n",
    "                self.logger.info(f\"Limiting processing to the first {limit} rows.\")\n",
    "                movies_df = movies_df.head(limit).copy()\n",
    "            self.stats['total_movies'] = len(movies_df)\n",
    "            \n",
    "            movies_df = self._initialize_columns(movies_df)\n",
    "            movies_df['_input_imdb_id'] = movies_df[imdb_id_column].astype(str) if imdb_id_column else np.nan\n",
    "            movies_df['extracted_year'] = movies_df[year_column].apply(self._extract_year) if year_column else np.nan\n",
    "            if year_column:\n",
    "                self.logger.info(f\"Year extraction complete with {movies_df['extracted_year'].notna().sum()} valid entries.\")\n",
    "\n",
    "        if movies_df is None:\n",
    "            raise RuntimeError(\"Failed to load or initialize DataFrame.\")\n",
    "        self.stats['_initial_processed_count'] = initial_processed_count\n",
    "\n",
    "        all_indices = set(movies_df.index)\n",
    "        indices_to_process = sorted(list(all_indices - self.stats['processed_indices']))\n",
    "\n",
    "        if not indices_to_process:\n",
    "            self.logger.info(\"No new movies to process. Saving current state.\")\n",
    "            for col in ['_input_imdb_id', 'extracted_year']:\n",
    "                if col in movies_df.columns:\n",
    "                    movies_df = movies_df.drop(columns=[col])\n",
    "            try:\n",
    "                movies_df.to_csv(output_path, index=False)\n",
    "                self._cleanup_checkpoints()\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving final data: {e}\")\n",
    "            return movies_df\n",
    "\n",
    "        self.logger.info(f\"Starting API enrichment for {len(indices_to_process)} movies.\")\n",
    "        self.progress_bar = tqdm(total=self.stats['total_movies'], initial=len(self.stats['processed_indices']),\n",
    "                                 desc=\"Enriching(API)\", unit=\"movie\")\n",
    "        processed_in_session = 0\n",
    "\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix='ApiEnricher') as executor:\n",
    "                futures_map: Dict[concurrent.futures.Future, int] = {}\n",
    "                indices_iter = iter(indices_to_process)\n",
    "                for _ in range(min(batch_size * 2, len(indices_to_process))):\n",
    "                    try:\n",
    "                        idx = next(indices_iter)\n",
    "                        row_data = movies_df.loc[idx]\n",
    "                        args = (idx, row_data, title_column, year_column)\n",
    "                        futures_map[executor.submit(self._process_movie, args)] = idx\n",
    "                        self.stats['attempted'] += 1\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                while futures_map:\n",
    "                    if self.is_paused:\n",
    "                        break\n",
    "                    done_futures, _ = concurrent.futures.wait(futures_map.keys(), return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                    for future in done_futures:\n",
    "                        idx = futures_map.pop(future)\n",
    "                        if idx in self.stats['processed_indices']:\n",
    "                            continue\n",
    "                        try:\n",
    "                            _, result_data, source = future.result()\n",
    "                            if result_data:\n",
    "                                for key, value in result_data.items():\n",
    "                                    if key in movies_df.columns:\n",
    "                                        movies_df.loc[idx, key] = value\n",
    "                            if source == 'skipped_no_title':\n",
    "                                self.stats['skipped_no_title'] += 1\n",
    "                            elif source == 'failed':\n",
    "                                self.stats['failed'] += 1\n",
    "                            else:\n",
    "                                stat_key = f\"success_{source}\"\n",
    "                                self.stats[stat_key] = self.stats.get(stat_key, 0) + 1\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error processing result idx {idx}: {e}\", exc_info=False)\n",
    "                            self.stats['failed'] += 1\n",
    "                        finally:\n",
    "                            self.stats['processed_indices'].add(idx)\n",
    "                            processed_in_session += 1\n",
    "                            self.progress_bar.update(1)\n",
    "                            if processed_in_session > 0 and processed_in_session % checkpoint_interval == 0:\n",
    "                                self._save_checkpoint(movies_df)\n",
    "                    if not self.is_paused:\n",
    "                        for _ in range(len(done_futures)):\n",
    "                            try:\n",
    "                                idx = next(indices_iter)\n",
    "                                row_data = movies_df.loc[idx]\n",
    "                                args = (idx, row_data, title_column, year_column)\n",
    "                                futures_map[executor.submit(self._process_movie, args)] = idx\n",
    "                                self.stats['attempted'] += 1\n",
    "                            except StopIteration:\n",
    "                                break\n",
    "                    if self.is_paused:\n",
    "                        break\n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.critical(\"Force exit requested.\")\n",
    "            self.is_paused = True\n",
    "        finally:\n",
    "            if self.progress_bar:\n",
    "                self.progress_bar.close()\n",
    "            signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "            for col in ['_input_imdb_id', 'extracted_year']:\n",
    "                if col in movies_df.columns:\n",
    "                    movies_df = movies_df.drop(columns=[col])\n",
    "            if self.is_paused:\n",
    "                self.logger.warning(\"Paused. Saving checkpoint.\")\n",
    "                self._save_checkpoint(movies_df)\n",
    "            else:\n",
    "                self.logger.info(\"Finished. Saving final data.\")\n",
    "                try:\n",
    "                    for col in self.NUMERIC_COLUMNS:\n",
    "                        if col in movies_df.columns:\n",
    "                            movies_df[col] = pd.to_numeric(movies_df[col], errors='coerce')\n",
    "                    movies_df.to_csv(output_path, index=False)\n",
    "                    self.logger.info(\"Dataset saved successfully.\")\n",
    "                    self._cleanup_checkpoints()\n",
    "                except Exception as e:\n",
    "                    self.logger.critical(f\"Failed to save final output: {e}\", exc_info=True)\n",
    "                    self.logger.warning(\"Data may be available in the checkpoint file.\")\n",
    "            self._log_final_stats(start_time)\n",
    "            if self.is_paused:\n",
    "                self.logger.warning(\"Paused. Resume by running the script again with the same arguments.\")\n",
    "        return movies_df\n",
    "\n",
    "    def _log_final_stats(self, start_time: float):\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        total_processed = len(self.stats['processed_indices'])\n",
    "        initial_count = self.stats.get('_initial_processed_count', 0)\n",
    "        processed_this_run = total_processed - initial_count\n",
    "        self.logger.info(\"\\n--- Enrichment Summary (API Only) ---\")\n",
    "        self.logger.info(f\"Total time: {duration:.2f} seconds\")\n",
    "        self.logger.info(f\"Dataset size: {self.stats['total_movies']}\")\n",
    "        self.logger.info(f\"Processed in this run: {processed_this_run}\")\n",
    "        self.logger.info(f\"Overall processed: {total_processed}/{self.stats['total_movies']}\")\n",
    "        if total_processed > 0:\n",
    "            success_total = self.stats['success_omdb_id'] + self.stats['success_omdb_title'] + self.stats['success_tmdb']\n",
    "            self.logger.info(\"API Success Breakdown:\")\n",
    "            omdb_id_count = self.stats.get('success_omdb_id', 0)\n",
    "            omdb_title_count = self.stats.get('success_omdb_title', 0)\n",
    "            tmdb_count = self.stats.get('success_tmdb', 0)\n",
    "            skip_count = self.stats.get('skipped_no_title', 0)\n",
    "            fail_count = self.stats.get('failed', 0)\n",
    "            if self.omdb_api_key:\n",
    "                omdb_id_perc = omdb_id_count / total_processed * 100 if total_processed else 0\n",
    "                omdb_title_perc = omdb_title_count / total_processed * 100 if total_processed else 0\n",
    "                self.logger.info(f\"  OMDb (by ID):    {omdb_id_count} ({omdb_id_perc:.1f}%)\")\n",
    "                self.logger.info(f\"  OMDb (by Title): {omdb_title_count} ({omdb_title_perc:.1f}%)\")\n",
    "            if self.tmdb_api_key:\n",
    "                tmdb_perc = tmdb_count / total_processed * 100 if total_processed else 0\n",
    "                self.logger.info(f\"  TMDb:            {tmdb_count} ({tmdb_perc:.1f}%)\")\n",
    "            success_perc = success_total / total_processed * 100 if total_processed else 0\n",
    "            skip_perc = skip_count / total_processed * 100 if total_processed else 0\n",
    "            fail_perc = fail_count / total_processed * 100 if total_processed else 0\n",
    "            self.logger.info(f\"  Total Success:      {success_total} ({success_perc:.1f}%)\")\n",
    "            self.logger.info(f\"  Skipped (No Title): {skip_count} ({skip_perc:.1f}%)\")\n",
    "            self.logger.info(f\"  Failed (Both APIs): {fail_count} ({fail_perc:.1f}%)\")\n",
    "        else:\n",
    "            self.logger.info(\"No movies were processed.\")\n",
    "        self.logger.info(\"---------------------------------------\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    INPUT_CSV_PATH = r\"C:\\Users\\divya\\OneDrive\\Documents\\GitHub\\spring-2025-greenlight\\Praneed(EDA&Preprocessing)\\data.csv\"\n",
    "    OUTPUT_CSV_PATH = r\"C:\\Users\\divya\\OneDrive\\Documents\\GitHub\\spring-2025-greenlight\\Praneed(EDA&Preprocessing)\\enriched_data.csv\"\n",
    "    TITLE_COLUMN_NAME = 'movie_title'\n",
    "    YEAR_COLUMN_NAME = 'release_date'\n",
    "    IMDB_ID_INPUT_COLUMN = 'imdb_id'\n",
    "\n",
    "    ROW_LIMIT = None  # Process all rows\n",
    "    MAX_WORKERS = 10\n",
    "    RATE_LIMIT = 10.0\n",
    "    CHECKPOINT_INTERVAL = 100\n",
    "    BATCH_SIZE = 100\n",
    "    FORCE_NO_RESUME = False\n",
    "\n",
    "    if not OMDB_API_KEY and not TMDB_API_KEY:\n",
    "        logging.critical(\"No API keys provided. Please set valid API keys in the script.\")\n",
    "    elif not OMDB_API_KEY:\n",
    "        logging.warning(\"OMDb API key not found. Using TMDb only.\")\n",
    "    elif not TMDB_API_KEY:\n",
    "        logging.warning(\"TMDb API key not found. Using OMDb only.\")\n",
    "\n",
    "    try:\n",
    "        enricher = MovieDataEnricher(\n",
    "            omdb_api_key=OMDB_API_KEY,\n",
    "            tmdb_api_key=TMDB_API_KEY,\n",
    "            max_workers=MAX_WORKERS,\n",
    "            rate_limit_per_second=RATE_LIMIT\n",
    "        )\n",
    "        enriched_df = enricher.enrich_dataset(\n",
    "            input_path=INPUT_CSV_PATH,\n",
    "            output_path=OUTPUT_CSV_PATH,\n",
    "            title_column=TITLE_COLUMN_NAME,\n",
    "            year_column=YEAR_COLUMN_NAME,\n",
    "            imdb_id_column=IMDB_ID_INPUT_COLUMN,\n",
    "            limit=ROW_LIMIT,\n",
    "            resume=not FORCE_NO_RESUME,\n",
    "            checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        logging.info(\"Script execution finished.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.critical(f\"Input file '{INPUT_CSV_PATH}' not found.\")\n",
    "    except ValueError as ve:\n",
    "        logging.critical(f\"Configuration or Data Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Unexpected error: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd. read_csv(r\"C:\\Users\\divya\\OneDrive\\Documents\\GitHub\\spring-2025-greenlight\\Praneed(EDA&Preprocessing)\\enriched_data.csv\")\n",
    "df = df.drop(columns=[\"final_genres\", \"final_runtime\", \"final_overview\", \"final_original_language\", \"final_rating\", \"final_production_countries\", \"cast\", \"crew\", \"director_id\", \"star_id\",\"final_certificate\" ], errors='ignore')\n",
    "df = df.dropna(subset=['rating', 'genres', 'runtime'])\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates} ({duplicates/len(df):.2%})\")\n",
    "\n",
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissing values by column (%):\")\n",
    "print(missing.apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "plt.tight_layout()\n",
    "plt.title('Missing Value Matrix (Yellow = Missing)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
